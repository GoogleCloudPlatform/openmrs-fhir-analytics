
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: '2.1'
services:
  batch:
    build:
      context: batch
    image: openmrs-fhir-analytics/batch-job:latest
    network_mode: host
    container_name: batch-cloud
    healthcheck:
      test: "exit 0"
    volumes:
      - ./utils/dbz_event_to_fhir_config.json:/usr/src/Main/dbz_event_to_fhir_config.json
      # data is the directory which you want to persist the generated parquet files
      - ./data:/tmp
    environment:
     - OPENMRS_URL=http://localhost:8099/openmrs
     - SEARCH_LIST=Patient,Encounter,Observation
     - OPENMRS_USERNAME=admin
     - OPENMRS_PASSWORD=Admin123
     - SINK_PATH=
     - SINK_USERNAME=
     - SINK_PASSWORD=
     - BATCH_SIZE=90
     - TARGET_PARALLELISM=5
     - JDBC_MODE_ENABLED=false
     - JDBC_DRIVER_CLASS=com.mysql.cj.jdbc.Driver
     - DB_USER=root
     - DB_PASSWORD=debezium
     - JDBC_FETCH_SIZE=10000
     - JDBC_MAX_POOL_SIZE=50
     - JDBC_INITIAL_POOL_SIZE=10
     - JDBC_URL=jdbc:mysql://localhost:3306/openmrs
     - NUM_PARQUET_SHARDS=3
     # the 2 variable below should be same as volume mappings above
     - PARQUET_PATH=/tmp/
     - FHIR_DEBEZIUM_CONFIG_PATH=/usr/src/Main/dbz_event_to_fhir_config.json

  streaming-binlog: # To run independently: docker-compose up --build streaming-binlog
    build:
      context: streaming-binlog
    image: openmrs-fhir-analytics/streaming-binlog:latest
    container_name: streaming-pipeline
    network_mode: host
    healthcheck:
      test: "exit 0"
    volumes:
      - ./utils/dbz_event_to_fhir_config.json:/deployments/dbz_event_to_fhir_config.json
      # data is the directory which you want to persist the generated parquet files
      - ./data:/tmp
    environment:
      - OPENMRS_URL=http://localhost:8099/openmrs
      - OPENMRS_USERNAME=admin
      - OPENMRS_PASSWORD=Admin123
      - SINK_URL=
      - SINK_USERNAME=
      - SINK_PASSWORD=
      - JDBC_FETCH_SIZE=10000
      - JDBC_MAX_POOL_SIZE=50
      - JDBC_INITIAL_POOL_SIZE=10
      - JDBC_URL=jdbc:mysql://localhost:3306/openmrs
      - JDBC_DRIVER_CLASS=com.mysql.cj.jdbc.Driver
      # the 2 variable below should be same as volume mappings above
      - PARQUET_PATH=/tmp/
      - FHIR_DEBEZIUM_CONFIG_PATH=/deployments/dbz_event_to_fhir_config.json

  streaming-atomfeed:
    container_name: streaming-atomfeed
    hostname: streaming-atomfeed
    build:
      context: ./
      dockerfile: ./streaming-atomfeed/Dockerfile
    healthcheck:
      test: "exit 0"
    environment:
      - OPENMRS_URL=http://localhost:8099/openmrs
      - SINK_URL=http://localhost:8098/fhir

  streaming-atomfeed-db:
    container_name: streaming-atomfeed-db
    hostname: streaming-atomfeed-db
    image: mysql:5.7
    command: "mysqld --character-set-server=utf8 --collation-server=utf8_general_ci"
    healthcheck:
      test: "exit 0"
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_USER=fhir
      - MYSQL_PASSWORD=fhiranalytics
      - MYSQL_DATABASE=atomfeed_client
      - MYSQL_ROOT_HOST=%
    volumes:
      - ./utils/dbdump/atomfeed_db_sql:/docker-entrypoint-initdb.d/dump.sql

  indicator-gen: # To run independently: docker-compose up --build indicator-gen
    build:
      context: dwh
    image: openmrs-fhir-analytics/dwh:latest
    container_name: indicator-gen
    network_mode: host
    healthcheck:
      test: "exit 0"
    volumes:
      # input: path to the parquet file
      - ./dwh/test_files:/dwh/input
      # output: path to indicator csv and other outputs
      - ./data/output:/dwh/output
      # indicator: path to indicator definition entry-point
      - ./dwh/indicators.py:/dwh/main.py
    environment:
      # PARQUET_PATH below should be consistent as volume mappings above (input)
      - PARQUET_PATH=/dwh/input
      - OUTPUT_CSV=indicators_output_XXXXXX.csv
      - NUM_DAYS=28
      - LAST_DATE=2020-12-30
      - CUSTOM_PARAMETERS=
